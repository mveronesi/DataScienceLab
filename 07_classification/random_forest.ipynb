{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing useful libraries\n",
    "import os\n",
    "import scipy.io.wavfile\n",
    "import scipy.fft\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold, ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample importing and feature extraction\n",
    "N_FOURIER = 3000 # number of features\n",
    "common_path_dev = \"../datasets/free-spoken-digit\" + os.path.sep + \"dev\"\n",
    "files_dev = os.listdir(common_path_dev)\n",
    "dataset_dev_features = np.zeros((len(files_dev), N_FOURIER), dtype=float)\n",
    "dataset_dev_labels = np.zeros((len(files_dev),), dtype=int)\n",
    "for i in range(len(files_dev)):\n",
    "    filepath = os.path.join(common_path_dev, files_dev[i])\n",
    "    dataset_dev_labels[i] = int(files_dev[i].split(\".\")[0][-1]) # takes class from name\n",
    "    with open(filepath, \"rb\") as file_bin:\n",
    "        tmp = scipy.io.wavfile.read(file_bin)[1]\n",
    "        dataset_dev_features[i, :] = np.power(np.abs(scipy.fft.fft(tmp, N_FOURIER)), 2)\n",
    "\n",
    "# creating evaluation set\n",
    "common_path_eval = \"../datasets/free-spoken-digit\" + os.path.sep + \"eval\"\n",
    "files_eval = os.listdir(common_path_eval)\n",
    "dataset_eval_features = np.zeros((len(files_eval), N_FOURIER, ), dtype=float)\n",
    "dataset_eval_numbers = np.zeros((len(files_eval),), dtype=int)\n",
    "for i in range(len(files_eval)):\n",
    "    filepath = os.path.join(common_path_eval, files_eval[i])\n",
    "    dataset_eval_numbers[i] = int(files_eval[i].split(\".\")[0]) # takes sample number from name\n",
    "    with open(filepath, \"rb\") as file_bin:\n",
    "        tmp = scipy.io.wavfile.read(file_bin)[1]\n",
    "        dataset_eval_features[i, :] = np.power(np.abs(scipy.fft.fft(tmp, N_FOURIER)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'n_estimators': 350}\n",
      "0.8734598775480431\n"
     ]
    }
   ],
   "source": [
    "# split train test & k-fold cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset_dev_features, dataset_dev_labels, test_size=0.2)\n",
    "kf = KFold(8)\n",
    "params = {\n",
    "    \"n_estimators\": [350],\n",
    "    \"max_depth\": [20, 30, 40]\n",
    "}\n",
    "best_config = None\n",
    "max_f1 = 0\n",
    "# finding best max_depth\n",
    "for config in ParameterGrid(params):\n",
    "    score = 0\n",
    "    n = 0\n",
    "    for train_indices, validation_indices in kf.split(X_train):\n",
    "        X_train_curr = X_train[train_indices, :]\n",
    "        X_valid = X_train[validation_indices, :]\n",
    "        y_train_curr = y_train[train_indices]\n",
    "        y_valid = y_train[validation_indices]\n",
    "        model = RandomForestClassifier(n_estimators=config[\"n_estimators\"], max_depth=config[\"max_depth\"], max_features=\"sqrt\")\n",
    "        model.fit(X_train_curr, y_train_curr)\n",
    "        y_pred = model.predict(X_valid)\n",
    "        score += f1_score(y_valid, y_pred, average=\"macro\")\n",
    "        n += 1\n",
    "    score /= n\n",
    "    if best_config == None or score > max_f1:\n",
    "        best_config = config\n",
    "        max_f1 = score\n",
    "\n",
    "print(best_config)\n",
    "print(max_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score:  0.88393438353178\n"
     ]
    }
   ],
   "source": [
    "# model testing\n",
    "model = RandomForestClassifier(n_estimators=400, max_depth=20, max_features=\"sqrt\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "print(\"Final score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final submission\n",
    "model.fit(dataset_dev_features, dataset_dev_labels) # fitting entire dataset\n",
    "output = np.zeros((dataset_eval_features.shape[0], 2), dtype=int)\n",
    "output[:, 0] = dataset_eval_numbers\n",
    "output[:, 1] = model.predict(dataset_eval_features)\n",
    "output = output[output[:, 0].argsort()]\n",
    "with open(\"predictions.csv\", \"w\") as f:\n",
    "    f.write(\"Id,Predicted\\n\")\n",
    "    for i in range(output.shape[0]):\n",
    "        f.write(str(output[i, 0]) + \",\" + str(output[i, 1]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd86e9477a69c659a08d66b84022f5f9bd15875679eb9c1dfdd3b3f73d8a5feb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dslab': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
